---
layout: post
title: Vision Transformer for Small-Size Datasets
categories: ['Paper Summary', 'Vision Transformer', 'Image Recognition']
---

**Title**: Vision Transformer for Small-Size Datasets ([Link](https://arxiv.org/pdf/2112.13492v1.pdf)) \
**Affiliation**: Seunghoon Lee et al., Inha University

![Overview](https://gcdn.pbrd.co/images/kD30VAjpBDSM.png?o=1)

#### Summary

ViT outperforms convolutional networks only when a large dataset is used for pre-training (JFT-300M), but underperforms when trained on small datasets due to lack of inductive bias. There are two reasons behind the decreased inductive bias - poor tokenization which limits the receptive field of each token, and poor attention mechanism which makes the attention score distribution smooth. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) that can be easily integrated to any ViT variants to improve performance. This paper improves the performance on small- and mid-sized datasets.

**Related work**: T2T-ViT, DeiT, CvT, CaiT \
mainly focus on mid-sized datasets, not small-sized datasets.

#### Key Components

1. Shifted Patch Tokenization


> embeds rich spatial information into visual tokens by increasing the receptive field of tokenization as much as spatially shifted.


As shown in the above diagram, the image is shifted in 4 different directions, concatenated with the original image, then are partitioned into patches. From my understanding, this results in 5x higher channel dimension to be projected to tokens. It is not specified whether the output dimension is maintained, but it seems much more computationally heavy. 

2. Locality Self-Attention Mechanism

> attenuates the smoothing phenomenon of the attention score distribution.

The first part of the LSA mechanism diagonal masking, which forces -&#8734; on diagonal components of the attention matrix i.e. makes ViT's attention more focused on inter-token attention instead of intra(self)-token attention. No ablation results are given as to whether diagonal masking alone shows any improvement, and whether self-attention is indeed redundant.

The second part is learnable scaling temperature for the attention matrix (result of dot product). The paper suggests that the original value used, &#8730;d<sub>k</sub> is too large. They empirically show that around half of the original value of optimal for CIFAR100 and TinyImageNet.

#### Personal reviews
* I could not find significant novelties. SPA just uses more information and compute, and LSA uses a learnable scaling parameter which is not new (AFAIK). While ignoring the diagonals of the attention matrix does seem like a new idea, the paper does not present sufficient ablation results or any consequences of neglecting intra-token attention. 
* Tables shows that using these modules results in lower throughput, higher FLOPs and higher params. In particular, higher FLOPs and params may not be suitable for smaller sized datasets.
* The improvement in performance varies across ViT variants, from negligible improvement to significant improvement. This shows that SPA and LSA are dependent on some conditions, which the paper fails to discuss.
* Still, the paper achieves improvements with simple techniques which is highly commendable.
