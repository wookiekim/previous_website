---
layout: post
title: Cost Aggregation Is All You Need for Few-Shot Segmentation
categories: ['Paper Summary', 'Semantic Correspondence', "Few Shot Segmentation"]
---

**Title**: Cost Aggregation Is All You Need for Few-Shot Segmentation ([Link](https://arxiv.org/abs/2112.11685)) \
**Affiliation**: Sunghwan Hong et al., Korea University - Arxiv 2021 (CVPR 2022?)

![Overview](https://gcdn.pbrd.co/images/s6sPOToDIeGO.png?o=1)
![Detailed Components](https://gcdn.pbrd.co/images/qioKe9m4zAWR.png?o=1)

#### Summary

This paper reformulates the few shot segmentation problem into a semantic correspondence problem, using both convolution and transformers to handle high-dimensional correlation maps between query and support.
The main contribution lies in the transformer-based modules (cost aggregation) which processes the correlation maps, which consists of a lot of (extended) novelties which were introduced in earlier works for efficient & effective attention mechanisms. The proposed method, Volumetric Aggregation with Transformers (VATs), achieves a new state-of-the-art on few shot segmentation and semantic correspondence.

**vs. Related work**: 
* HSNet is the main paper to be compared against, they have a very similar pipeline
    * computing correlation map from concatenation of all feature maps with same spatial size
    * pyramidal manner processing to let the coarser level guide the finer level
* However, HSNet lacks ability to consider interaction between matching scores due to the inherent nature of convolutions.
    * However, other studies show that early layer convolutions 'help transformers see better', so this work uses a combination of those two.

#### Key Components
The overall pipeline is shown in the first figure, and the outline of Volumetric Embedding Module (VEM) and Volume Transformer Module (VTM) are shown in the first figure. The feature extraction and computation is not that different from that of HSNet.

###### Volumetric Embedding Module (VEM)
HWHW tokens are just too many for transformer to process, so VEM reduces the number of tokens while injecting 'convolutional bias' by applying 4D spatial maxpooling - overlapping 4D convolutions - ReLU - Group Normalization - projection from each 4D position to sequentially reduce support and query spatial dimensions.

###### Volume Transformer Module (VTM)
Concretely, an attention-based transformer encoder. Extends 2D swin transformer to 4D with residual connection.

###### Affinity-aware Transformer Decoder
Additionally utilize the appearance embedding obtained from query feature maps together with the hypercorrelation. From my understanding, similar to what was done in CATs (Semantic correspondence).

#### Personal reviews
* a novel combination of existing techniques (ex. 2D -> 4D Swin).
* Ablation study does show significance of each module, but I wonder why the ablation was not carried out on PASCAL-5 or COCO-20, but instead on FSS-1000.
* conv4_x is excluded when trained on PASCAL-5i and COCO-20, but it is not explained why. I have to suspect that it shows lower results due to (perhaps) overfitting.
* like CATs, this work uses data augmentation in its work unlike other methods, and do not report its pre-augmentation results. I do not think this is a fair comparison. Either the baselines should be evaluated with augmentation, or VAT without augmentation. 
* In the cost aggregator ablation study, standard transformers are included, but not 2D-2D separated transformers as in CATs. Since they are the authors of CATs, I believe it would have been attempted.
* FLOPs/MAC not included.
* I'm not certain if the statement "It is well known that the number of parameters has inverse relation to generalization power." is well applicable/justifiable in this case, especially so when data augmentation was used only in this paper.
