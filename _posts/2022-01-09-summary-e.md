---
layout: post
title: Splicing ViT Features for Semantic Appearance Transfer
categories: ['Paper Summary', 'Continual Learning']
---

**Title**: Splicing ViT Features for Semantic Appearance Transfer ([Link](https://arxiv.org/abs/2201.00424v1)) \
**Affiliation**: Narek Tumanyan, Omer Bar-Tal et al., Weizmann AI Center - Arxiv 2021

![Overview](https://gcdn.pbrd.co/images/ZKhmcneboRDY.png?o=1)
![Architecture](https://gcdn.pbrd.co/images/KwZBh5xpg5de.png?o=1)

#### Summary

Semantic style transfer, but without additional cues such as semantic mask / correspondences. Maintains the structure of the source image, but the target style (semantics) are transferred. This is a new task proposed by this paper. The disentanglement of structure and semantic features are done by leverating the Dino-ViT architecture. The generator takes as input a single source image such that it has the semantics of the target image - so a single generator is trained for a single pair of images, and therefore a generalizable model would be the future research direction. Interesting qualitative results. SoTA on human evaluation and IoU (structure maintained well).

**vs. Related work**: 
* Domain Transfer / Image2Image translation
  * learn domain-to-domain, mostly use GAN.
  * proposed task learns image-to-image, uses image generator
* Neural Style Transfer
  * usually transfers global artistic style, but proposed task transfers the **appearance** of semantically related objects

#### Key Components

Main observation & motivation, Dino-ViT:
* [CLS] token encodes appearance information in a spatially flexible manner (observed from inversion)
* The Key tokens from the final layer can also reconstruct the image when inverted
* PCA of keys' self-similarity mostly capture semantic scene/object parts while discarding appearance information

###### Appearance Loss
Difference in CLS token between the generated image and the texture (target) image, L2 Norm. Refer to 2nd figure for details

###### Structure Loss
Difference in self-similarity of the keys extracted from the deepest layer of the attention module (Dino ViT, pretrained) of the source image and output image


###### Identity Loss
Encourages generator to preserve the keys representation of the **target** image. Used as regularization.

#### Dataset
Animal Faces HQ, Flickr Mountain, Pixabay

#### Personal reviews
* Couldn't understand how the regularization term works. The image input to the generator is the source image, but the identity loss is calculated w.r.t the target image (and the generated target image). Why not use the source image?
* The qualitatives results are very interesting, and has made good use of Dino-ViT. It would be interesting (and helpful) to find out about these differences between ViT variants and convolutional variants, mostly regarding how their appearance/structural encoding differs.
* The ideas introduced here may be useful for semantic correspondence as well, especially how they decouple semantic and structural appearances just by leverating Dino-ViT features.
* The losses introduced seem very simple. Why this is a very good point (in my POV), if these ideas (CLS tokens, key tokens self-similarity...) have been proposed before, the paper's novelty could be questioned.