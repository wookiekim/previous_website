---
layout: post
title: Pruning Self-Attentinos into Convolutional Layers in Single Path
categories: ['Paper Summary', 'Vision Transformer']
---

**Title**: Pruning Self-Attentions into Convolutional Layers in Single Path ([Link](https://arxiv.org/abs/2111.11802)) \
**Affiliation**: Haoyu He et al., Monash University - Arxiv 2021

![Overview](https://gcdn.pbrd.co/images/O0ubNINxIQw5.png?o=1)

#### Summary

Transformers suffer largely from a) large computation and b) lack of inductive biases. Existing studies aimed to solve this problem by integrating transformers with convolutional architectures in a heuristic manner, or heavy NAS. This paper shows that a learned MSA can be formulated into a convolutional layer, and lets the network learn to choose to run MSA or convolution in every layer. This results in favorable accuracy-efficiency trade-off. This paper also confirms that last few FFN layers have more redundancy, and that locality is encouraged in shallow blocks.

#### Key Components

###### Weight-sharing between MSA and convolutional operations
Proposes a way to use MSA weights to formulate them into convolutional operations. 
* Ex) Fix the attention for non-local correlations to 0, since convolution operations takes place locally. 

###### Single-path vision transfer pruning
Use sigmoid(learnable param) to determine the likelihood for choosing the p-th operation (from skip connection / MSA / convolution). 
* To get architectures with desired efficiency constraints, use an auxiliary compuitational complexity loss.

#### Dataset
ImageNet-1K

#### Personal reviews
* The ideation is not novel, but the implementation of integrating MSA and convolution in an interchangeable manner is interesting. Improvements could be made, but this paper did show that convolutional and MSA can be changed interchangeably, while they do have different characteristics.
* It is interesting that the weights of MSA (without inductive biases) are suitable starting weights for convolutions (with inductive biases). 
* The proposed interchangeable scheme is applicable for any vision transformer architectures (DeiT, Swin).
* The single-path vision transfer pruning seems too simplistic, in the way that the resulting discussions or takeaways are not very different from those of heuristic conv+attention approaches.