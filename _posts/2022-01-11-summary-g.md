---
layout: post
title: Sparse DETR; Efficient End-to-End object detection with learnable sparsity
categories: ['Paper Summary', 'Vision Transformer', 'Object Detection', 'Sparse Transformer']
---

**Title**: Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity ([Link]()) \
**Affiliation**: Byungseok Roh, Jaewoong Shin et al., KakaoBrain, Lunit - Submitted to ICLR 2022

![Overview](https://gcdn.pbrd.co/images/RU09D06Np7Xe.png?o=1)
![Architecture](https://gcdn.pbrd.co/images/bOS0neCcc6vj.png?o=1)

#### Summary

DETR suffered computational costs, which was mainly addressed in Deformable DETR. However, the deformable attention enabled the usage of multiscale features (during extraction), which increased the costs again and the bottleneck still exists, mainly due to the much larger (20x) number of tokens. The authors observe that detection performance hardly deteroirates even if only a part of the encoder token is updated. The proposed work, Sparse DETR, selectively updates tokens. This lightens the layer-wise computation, enabling auxiliary detection losses to be applied on the selected tokens every layer. Results show ~40% decrease in cost, ~40% increased in FPS while showing better performance than Deformable DETR.

#### Key Components

###### Encoder Token Sparsification
Decoder Cross-Attention Map
* Cross-attention maps from the transformer decoder used to measure saliency
  * Aggregate decoder cross-attentions between all object queries and the encoder output 
  * used to supervise the "scoring network" for salient token prediction
    * Scoring network: 4-layer network, supervised using BCE loss.
  * binarize the Cross-attention map such that only the top-p% of encoder tokens is only retained.
* Unlike sparse-NCNet, the non-salient regions are just passed straight through, and are not discarded.
  * So they can still be involved in the query-key interaction calculations
  * Which is necessary since this paper uses deformable attention after sparsification.
  * This seems to be a much better design choice in comparison to naive discarding

###### Additional Components

Encoder Auxiliary Loss
* Sparse DETR: only part of encoder tokens are refined by the encoder
* Not much computational burden in adding auxiliary heads only for sparsified tokens
* Adding losses at intermediate layers helps distinguish confusing features in the decoder.

Top-k Decoder Queries
* Instead of learnable queries for decoder, uses top-k encoder outputs (based on score) as decoder queries
* Not really a theoretical explanation for doing so, but shows empirically better results

#### Key Results / Takeaways
Swin-T shows noticeable improvement even under extreme sparsity unlike ResNet variants
* conjecture that single token in Swin-T can hold a wider region of information than one in ResNet
* the network seems to have enough information even after aggressive sprsification

Dynamic sparsification (changing the keeping ratio at test time)
* graceful decrease in performance for higher sparsification
* But still shows OK performance, unlike PnP DETR, which suffers significant performance degradation when using dynamic sparsification WITHOUT further tricks.

#### Dataset
COCO

#### Personal reviews
* Great motivation, the method of inducing sparsity is not very inspiring, but persuasive. 
* The idea of handling non-salient tokens is interesting.
* Just like how deformable DETR uses multiscale features which was enabled due to reduced compute from deformable attn, this paper proposes to use auxiliary losses which is OK due to sparsified tokens. Still results in higher FPS and lower compute compared to deformable DETR.
* The reason for using top-k encoder outputs as decoder inputs is not clarified in a theoretical manner, but is still an intersting finding that could easily be applied in other DETR-variants.