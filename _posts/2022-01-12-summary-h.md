---
layout: post
title: A ConvNet for the 2020s
categories: ['Paper Summary', 'Vision Transformer', 'CNN']
permalink: /test_permalink/
---

**Title**: A ConvNet for the 2020s ([Link](https://arxiv.org/abs/2201.03545)) \
**Affiliation**: Zhuang Liu et al., FAIR, UC Berkeley - Arxiv 2022

![Overview](https://gcdn.pbrd.co/images/65oXyfJC9L8K.png?o=1)
![Odyssey](https://gcdn.pbrd.co/images/c7PsSmkcORet.png?o=1)

#### Summary

The paper aims to reexamine the design spaces and test the limits of what a pure ConvNet can achieve, in today's trend of using Transformers. The authors gradually modernize a standard ResNet toward the design of a vision transformer, and shows sevel key compoenents that contribute to the performance difference along the way. The final outcome is a family of ConvNet models named **ConvNext**, competing favorably with Transformers in terms of accuracy and scalability in image classification/segmentation/detection. 

#### Key Components

All proposed design changes are results of trying to get closer to ViT / vision transformer designs from a regular ResNet.

###### Macro Design
* Changing stage compute ratio from (3,4,6,3) to (3,3,9,3) as in transformers show better results.
* Replacing ResNet-style stem cell with a patchify layer (kernel size 4, stride 4) shows better results.
* Increase the network width to same number of channels as Swin-Ts (64->96), which is enabled by using depthwise convolution which reduces the FLOPS and a little accuracy. This is referred to as ResNeXt-ifying.
* Inverted bottleneck design shows better results.
* Moving up the depthwise conv layer, so that the kernel size can be increased (as in transformers) without infeasible computation overhead. Benefit of large kernel size reaches a saturation at 7x7.

###### Macro Design
* Replacing ReLU with GELU shows better results.
* Fewer normalization layers, by removing two BN layers to leave only one BN layer before the conv1x1 layers. This show better results, already surpassig Swin-T's performance.
* Substituting BN with LN shows slightly better results.
* Add separate downsampling layers (WITH NORMALIZATION to stabilize trainig, otherwise diverges) for better results.

#### Dataset
ImageNet (1K, 22K), COCO dataset, ADE20K dataset

#### Personal reviews
* As the paper says, the proposed schemes are not novel - but they have not been considered collectively before, and the takeaway that convolutional networks can be "just as good" seems valuable.
* Evaluation on downstream tasks were provided (a big plus).
* I wonder how the 'steps' in assigning macro/micro design changes were applied. The step of applying macro/micro design changes one by one seems to assume that each change is independent of one another, which may not always be the case. For example, what if ResNeXt-ifying was done before changing the compute ratio? It would ofc be impossible to include the results of all possible orders, but I wonder if there was a certain rule the authors abided by.
* Fixing the training scheme was a very large plus in my opinion as well. Many papers use their own scheme, which I think may always be an unfair setting.