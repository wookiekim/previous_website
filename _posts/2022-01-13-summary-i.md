---
layout: post
title: SPViT; Enabling Faster Vision Transformers via Soft Token Pruning
categories: ['Paper Summary', 'Vision Transformer', 'Sparse Transformer']
---

**Title**: Enabling Faster Vision Transformers via Soft Token Pruning ([Link](https://arxiv.org/abs/2112.13890)) 
**Affiliation**: Zhenglun Kong, Peiyan Dong et al., Northeastern University - Arxiv 2021

![Overview](https://gcdn.pbrd.co/images/NQaDxTm1JW2W.png?o=1)
![TokenSelector](https://gcdn.pbrd.co/images/a3EZNuvgQ4CF.png?o=1)

#### Summary

Transformers suffer largely from large computation. Leveraging the input token sparsity in transformers, the paper proposes a computation-aware soft-pruning network. The term 'soft'-pruning is derived from the implementation where the non-salient tokens are not fully discarded, but are packaged into a single token so that the information does remain within the network. The proposed SPViT significantly reduces compute while maintaining comparable performance, and achieves real-time execution of DeiT-T on mobile platforms (~40% superior than existing work).

#### Key Components

###### Attention-based Multi-head Token Selector
Tokens behave differently in each head
* calculate per-head scores
* calculate per-token scores for each head
* aggreagate for final per-token scores
* Use them for pruning

##### Token Packaging technique
Completely removing background tokens may weaken self attention's ability to capture key information / Poor scoring may cause important tokens to be removed
* Less informative tokens are combined into one token, weighted by their token scores.

###### Computation-Aware Training Strategy
* Computation-aware sparsity loss to obtain pruning rate constrained by computation resources of target device
* Layer-to-phase progressive training schedule by which we can determine the location of inserted selectors and their suitable pruning rates.

#### Dataset
ImageNet

#### Personal reviews
* The idea that 'removing background tokens weakens self-attention's ability to capture key information' does have references, but is not contained within the paper. The ablation results are shown, but providing more detailed theoretical explanations could have been better.
* The idea of aggregating nonimportant tokens is inspiring, but they may not be applicable for downstream tasks requiring dense predictions (ex segmentation).
* The layer-to-phase progressive training schedule involves "repeating insertion until there is one selector for each block", where "for each insertion the current selector is trained while other parts are finetuned". To me, this sounds like the training is going to take a lot of time and engineering.