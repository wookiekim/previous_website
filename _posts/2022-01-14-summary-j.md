---
layout: post
title: Evo-ViT;Slow-Fast Token Evolution for Dynamic Vision Transformer
categories: ['Paper Summary', 'Vision Transformer', 'Sparse Transformer']
---

**Title**: Evo-ViT:Slow-Fast Token Evolution for Dynamic Vision Transformer ([Link]()) 
**Affiliation**: Yifan Xu, Zhijie Zhang et al., Chinese Academy of Sciences/Jiaotong U - Arxiv 2021

![Comparison](https://gcdn.pbrd.co/images/OjidJ1njsNnp.png?o=1)
![Overview](https://gcdn.pbrd.co/images/Hl0DYSmtEoSr.png?o=1)

#### Summary

Transformers suffer largely from large computation. Leveraging the input token sparsity in transformers, many studies aim to prune transforemr networks, but they have largely 2 limitations: a) incomplete spatial structure caused by pruning is not compatible with structured spatial compression, and b) usually requires a time-consuming pre-training procedure. This paper proposes Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Even non-salient tokens are not discarded, but are updated "fast", while salient tokens are updated "slowly" by MSA and FFN. The saliency of tokens are determined by the similarity scores between the CLS token and patch tokens. 

Note that the first diagram compares the proposed method with existing work. The third branch depicts the proposed work, while the above two branches show existing work. Remark that the proposed method does not rely on pretraining.

#### Key Components

###### Structure-preserving token selection
Simply put, the structure is preserved because the tokens are not discarded / summarized into a single token.
* Unlike Sparse DETR, the non-salient tokens are still updated (in a similar manner to SPViT, by summarized non-salient tokens)
* Use centered kernel alignment similarity to measure similarity of intermediate token features in each layer and the final CLS token.
* Different sparsification scheme for different instances (input image)

##### Slow-fast token updating
Salient tokens: Slow update
* CLS token + salient tokens + summarized non-salient tokens
* MSA and FFN, as in normal transformers

Non-salient tokens: Fast update
* Residual connection with "summarized non-salient token" output from above step.

Results are aggregated together for structure preservation.

###### Layer-to-stage traiing schedule
First prune for each layer (first 200 epochs), later only leave the token selection layer at the beginning of each **stage** (4 layers). 
* Similar to SPViT

###### Assisted CLS token loss
In vision transformer for classification, classification is either performed on the classification head, or on the average pulled features.
* Uses both outputs.

#### Dataset
ImageNet

#### Personal reviews
* Similar to SPViT, but better in my own opinion in that the structure is preserved. Also, compared to Sparse DETR, the non-salient regions are updated as well.
* While the current setting relies on CLS token similarity scores, the scheme used in Sparse DETR (DAM, Cross attention between keys at decoder) could be used for tasks other than classification (since in dense prediction task, relying on CLS token similarity would be misleading).