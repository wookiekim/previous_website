---
layout: post
title: GLIDE; Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
categories: ['Paper Summary', 'Diffusion Model', 'Image Generation']
permalink: /summary_glide/
---

**Title**: GLIDE; Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models ([Link](https://arxiv.org/abs/2112.10741)) \
**Affiliation**: Alex Nichol et al., OpenAI - Arxiv 2021

![Overview](https://gcdn.pbrd.co/images/xPauChFJtijq.png?o=1)

#### Summary
Diffusion models have recently proved to be able to generate high-quality synthetic images, especially when paired with a guidance technique (while trading off diversity). 
This paper applies guided diffusion to the problem of text-conditional image synthesis. This paper also compares between CLIP guidance and classifier-free guidance, showing that classifier-free creates higher quality images. 
The proposed model can also be fine-tuned to perform image inpainting for powerful text-driven image editing. 
A smaller model (trained on a filtered dataset) is released on github.

#### Key Components
Note that I am not very familiar with image synthesis or GAN, let alone diffusion models. I could not fully understand the technical details behind this paper, but the qualitative results are still very interesting.

* 3.5B param text-conditional diffusion model at 64x64 resolution
  * same dataset as DALL-E
  * 2.3B params for visual part of the model
  * 1.2B params for text encoding transformer
* 1.5B param text-conditional upsampling diffusion model (64x64 -> 256x256)
* a 64x64 ViT-L CLIP model for CLIP guidance

<br>

* Base model trained for **2.5M** iterations at batch size 2048.
* upsampling model trained for **1.6M** iterations at batch size 512.
* Fine-tuning for classifier-free guidance: 2-% of token sequences are replaced with empty, so that model retains its ability to generate text-conditional outputs, but can also generate images unconditionally
* Image inpainting: model is finetuned such that random regions of training examples are erased, and remaining portions are fed into the model.
  * Upsampling model is given **full** low-resolution image, but only the unmasked region of high-resolution image

<br>

* Qualitative & quantitative results are great
* To prevent harmful impacts of releasing this models (unskilled users can quickly make convincing edits to existing images ex. DeepFake)
  * filtered training images before training models for release
  * filtered out images containing people to reduce people-centric problematic use
  * filter out image that could produce violent/hate images



#### Personal reviews
* I can't really say much about the technical novelty or contribution, as I am not very familiar with this field. However, it was visible that the proposed "concepts" were not new, but their "empirical application" could be a novelty as the proposed pipeline requires a lot of GPU resources to train.
* The qualitative results are great, and quantitative results seem to be okay as well. When comparing the qualitative results with other methods, I actually couldn't decide which output was the best.
* Since the figure attached in this article shows the cherry-picked qualitative results, I am curious whether there are completely crappy output as well. 
* While a discussed limitation was that the model "sometimes failed to capture certain prompts which describe highly unusual objects or scenarios", the attached figure shows that it succeeded in creating a "hedgehog using a calculator". I wonder where the learned model crosses the line between "succeeding" and "failing" to catch a prompt.
* The paper does an excellent job by releasing the model which was trained on the "filtered" dataset, so that unskilled users would not be able to exploit the model to create hateful/pornographic content. 